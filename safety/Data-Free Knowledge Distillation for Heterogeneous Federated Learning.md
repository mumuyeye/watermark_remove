# Data-Free Knowledge Distillation for Heterogeneous Federated Learning——异质性联邦学习与无数据知识蒸馏

### Abstract

- 联邦学习(FL)是一种分散的机器学习范式，其中全局服务器迭代地聚合本地用户的模型参数，而无需访问其数据
  - **用户的异质性**给FL带来了巨大的挑战，这可能导致缓慢收敛的漂移全局模型
    - 用户数据的分布不均匀或者不独立同分布
- 可以使用**知识蒸馏**来解决这个问题，通过使用来自异构用户的聚合知识来精炼服务器模型，而不是直接聚合他们的模型参数
  - 这种方法**依赖于代理数据集**
  - **没有充分利用集成知识**来指导局部模型学习，这可能会影响聚合模型的质量
- **无数据的知识蒸馏方法**
  - 其中服务器学习轻量级生成器以无数据的方式集成用户信息，然后将其广播给用户，使用学习到的知识作为归纳偏差来调节本地训练

### Introduction——FEDGEN

<img src="C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230729204227969.png" alt="image-20230729204227969" style="zoom:67%;" />

- 让服务器根据用户的模型预测结果，学习一个可以生成合成数据的模型，这个模型可以反映用户之间的共同知识
  - 服务器把这个生成模型发送给用户，让用户在本地训练时，用生成模型产生的合成数据来辅助自己的真实数据，从而提高模型的性能和效率
- `FEDGEN`拥有以下优点
  - 它**不需要任何真实的用户数据**，只需要用户模型的预测结果，这样可以保护用户数据的隐私和安全
    - data-free
  - 它可以利用生成模型来**提取和传递用户之间的知识**，而不是简单地平均用户的模型参数，这样可以避免知识的损失和冲突
  - 它可以直接用生成模型来指导用户在本地训练自己的模型，而不是只对全局模型进行优化，这样可以增强用户模型的泛化能力和稳定性
  - 它可以设计一个轻量级的生成模型，只需要一个比输入空间小得多的潜在空间，这样可以减少服务器和用户之间的通信开销和计算开销

#### Algorithm 1 FEDGEN

- 首先，需要一个中心服务器和一些本地设备（或客户端），它们都有一个初始的全局模型参数θ。
- 然后，重复以下的过程：
  - 中心服务器随机选择一些设备参与本轮的训练。
  - 中心服务器把全局模型参数θ发送给这些设备。
  - 每个设备根据自己的本地数据集和一个生成器，训练自己的本地模型参数θk，并且计算自己的用户信息uk。
  - 每个设备把自己的本地模型参数θk和用户信息uk发送回中心服务器。
  - 中心服务器根据所有设备的本地模型参数和用户信息，更新全局模型参数θ。



#### 