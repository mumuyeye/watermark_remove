# 20230731

## A Systematic Review on Model Watermarking for Neural Networks

### Background

- model stealing
  - 对抗性样本攻击
- model extraction attack
- backdoors in NNs
  - 一种有意训练ML模型,以在给定的一组训练数据点上输出错误预测的技术
  - over-parametrization

### Taxonomy of nn watermarking

1)**嵌入方法**:指将水印嵌入到模型中的方法

- 将水印或相关信息直接**插入模型参数**中
  - 位串、附加参数
- 创建触发器、载体或关键数据集，该数据集**由标记模型中引起异常预测行为的数据点**组成
  - 在模型中**插入一个后门**，使模型学会对来自触发数据集的数据点表现出不寻常的预测行为
    - 由合法所有者查询触发数据集，通过阈值计算判定是否盗取
  - 触发数据集的来源？

<img src="C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230731223306735.png" alt="image-20230731223306735" style="zoom:67%;" />

<img src="C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230731223626746.png" alt="image-20230731223626746" style="zoom:67%;" />

2)验证访问:指定水印的**验证方式**，可以通过白盒访问，也可以通过黑盒访问

3)容量:区分zero-bit或multi-bit方案。前者是指不携带**额外信息**的水印，而后者是指携带额外信息的水印

### CATEGORIZING WATERMARKING METHODS

#### Embedding Watermarks Into Model Parameters

- 将信息包含在模型参数或参数符号的最低有效位中

- 将水印解释为一个T位字符串$\{0,1\}^T$，使用复合损失函数，在某些模型参数上施加统计偏差

- 在原始模型之外使用了一个额外的独立神经网络来投影水印，在输出向量和水印之间应用二值交叉熵损失

- 将带有数字签名的passport-layers嵌入到神经网络中

#### Using Pre-Defined Inputs as Triggers

- 识别非常接近决策边界的对抗样本和正常数据点
  - 触发数据集由50%的对抗性示例和50%不会导致误分类但接近决策边界的数据点组成
  - 对训练好的分类器进行微调，以将触发数据点预测到正确的原始类
    - 完整性较弱

<img src="C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230801102433183.png" alt="image-20230801102433183" style="zoom:67%;" />

- 通过改变模型不同层次的数据抽象的概率密度函数，来嵌入一个由T位组成的字符串作为水印
  - 可以在不同的网络层中嵌入不同的水印字符串
  - 使得选定的分布的均值包含水印信息
- 构建了一个模型相关的编码方案，根据模型输出激活的相似度，将它们分成两组，一组为类别0，一组为类别1
  - 将二进制签名(作为水印)包含在模型的输出激活中，并可以通过指定的触发数据集进行验证

##### Trigger Dataset Creation Based on Original Training Data

##### Robust Watermarking

- 使用原始训练数据集之外的水印方法易被去除水印，且对模型的效果影响不大
  - 从水印触发数据集中学习到的信息是冗余的，并且独立于主任务
- 使用包含水印信息的附加NN对原始NN进行正则化，使水印信息嵌入到与主要分类任务相同的模型参数中
- 纠缠水印嵌入
  - 提取代表原始任务的数据和编码水印的数据的共同特征
  - 确保水印和原始任务由相同的子模型表示
- 指数加权
  - 从训练分布中随机抽样并为训练样本分配错误标签来生成水印触发器，但对于触发样本予以更大的训练权重

## Robust Multi-bit Natural Language Watermarking through Invariant Features

- 使用提取模型从原始文本X中提取出状态S。状态S是文本的不变特征的函数，它包含了足够的信息来决定水印的嵌入和提取过程
  - 如果不变特征是标点符号，那么状态S就是文本中每个句子的结尾是否有逗号
- 使用编码方案将水印信息编码为一系列的0和1，并根据不同的不变特征进行嵌入，得到一个带有掩码（mask）的文本 ̃Xwm
  - 掩码是指需要插入或删除单词的位置
  - 如果不变特征是标点符号，那么可以在句尾添加或删除逗号来表示0或1，并在相应的位置添加掩码
- 使用填充模型根据水印信息和状态S，在掩码处生成合适的单词，得到水印文本Xwm
  - 填充模型是通过对抗训练得到的
  - 如果不变特征是标点符号，那么可以在句尾添加或删除逗号来表示0或1，并在相应的位置生成一个与原文语义相近但不完全相同的单词

## Undeectable Watermarks for Language Models

- 受密码学启发
  - 只有在知道密钥的情况下才能检测到水印;在没有密钥的情况下，在计算上难以区分带水印的输出和原始模型的输出
- 选择一个随机的秘密密钥k，以及一个单向函数f，即一个容易计算但难以逆向的函数
- 对于每个可能的输入x，计算f(k,x)，并将其作为一个隐秘的标签t
  - 水印模型可以应对任何用户选择的输入x，而不需要预先设定一个固定的输入集合
- 修改语言模型，使得它在输出y时，根据t来调整y中某些单词的概率。例如，如果t是偶数，就增加y中偶数位置单词的概率；如果t是奇数，就增加y中奇数位置单词的概率。这样，水印就被隐蔽地嵌入了y中
- 为了检测水印，只需用k和x重新计算t，并根据t来分析y中单词的概率分布。如果y中单词的概率分布与t相匹配，就说明y是由水印模型生成的；否则，就说明y是由原始模型或其他模型生成的

## Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding

- 将水印信息W编码成一个二进制串B，然后将B分割成若干个子串B1,B2,…,Bn，每个子串的长度为k位
- 使用一个编码器-解码器模型，给定一个输入文本T和一个子串Bi，生成一个输出文本Ti，其中Ti与T相似，但是在某些位置上有一些词语的替换。这些替换的位置和方式是由Bi决定的，从而实现水印嵌入
  - **编码器**负责将输入文本T编码成一个隐藏向量H，**解码器**负责将隐藏向量H和水印子串Bi解码成一个输出文本Ti。编码器和解码器都是由多层的自注意力机制和前馈神经网络组成(**Transformer**)
  - 模型使用了一个特殊的**损失函数**，它包括两个部分：一个是**重构损失**，它衡量T和Ti之间的语义相似度；另一个是**对比损失**，它衡量T和Ti之间的语法差异度

- 使用一个对抗训练的方法，训练一个判别器来检测文本中是否有水印，并给出反馈信号给编码器-解码器模型，使其生成更隐蔽的水印文本
- 将所有的输出文本Ti拼接起来，得到带有水印信息W的文本T'

## Ideas

- 是否可以思考出第三类嵌入方法？或融合上述两类
- 对于嵌入方法的改良与创新
  - 使用自编码器或变分自编码器
    - 添加新的layer或在原有layer中做水印的处理
  - 插入水印信息的新方法？
  - 改变触发集的选择方式？