# 水印的去除

## 通常的去除方案

### Fine-Tuning

- REFIT: A Unified Watermark Removal Framework For Deep Learning Systems With Limited Data
  - 攻击者只有有限的训练数据，基于微调技术，不需要知道水印的具体内容
    - 弱攻击模型
    - 对模型的准确性影响不大
  - 一种改进的弹性权重整合（EWC）算法，它最初是用于缓解灾难性遗忘（catastrophic forgetting）现象的
    - 灾难性遗忘是指神经网络在学习新任务时，会忘记之前学习的任务的知识
    - EWC算法的思想是，在学习新任务时，对之前任务中重要的参数施加一个惩罚项，使它们不容易改变
      - 这样就可以保留旧任务的知识，同时又能适应新任务的需求(遗忘水印)
  - 一种无标签数据增强（AU）方法，它利用了来自其他来源的无标签辅助数据
    - AU方法的思想是，在无标签数据上进行一些变换，比如替换、删除、插入、回译等，生成一些新的数据样本，并用模型对它们进行预测
      - 根据模型对原始数据和变换后数据的预测结果之间的一致性，计算一个额外的损失项，用于优化模型
- Detect and remove watermark in deep neural networks via generative adversarial networks
  - 第一阶段，利用生成对抗网络（GAN）和少量干净的图像检测出DNN模型中的水印类别和水印触发器，并将其反转。具体来说，首先利用GAN生成一些与原始数据集相似但不完全相同的图像，然后将这些图像输入到DNN模型中，观察模型的输出。如果模型输出了一个与原始数据集不同的类别，那么就说明这个类别是水印类别，这个图像是水印触发器。然后，将水印触发器进行反转，即将其像素值取反，得到反转后的图像
  - 第二阶段，利用反转后的图像对水印DNN进行微调，从而去除后门水印。具体来说，首先将反转后的图像与原始数据集中相同类别的图像混合，形成一个新的数据集。然后，利用这个新的数据集对DNN模型进行微调，使模型在反转后的图像上输出正确的类别，从而消除水印效果        

### Pruning

- Protecting Intellectual Property of Deep Neural Networks with Watermarking
- 去掉一些冗余的参数，得到一个看起来与原模型不同，但仍然具有同样高的预测精度的新模型

  - 如果包含水印的参数被去除，则水印将无法验证


#### Fine-Pruning

- Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks
- 先修剪模型架构，然后继续微调
  - 去除可能被水印影响的神经元，然后对修剪后的网络进行微调，以恢复其性能
  - 微调时需要添加噪声使得水印去除更加有效

### Rounding

- Effectiveness of Distillation Attack and Countermeasure on Neural Network Watermarking
- 降低模型参数的精度
  - 如果模型强烈过拟合作为水印的触发器数据，或者直接将水印包含在参数中
- 仅针对基于参数修改的水印算法

### Distillation

- Effectiveness of Distillation Attack and Countermeasure on Neural Network Watermarking
- 水印信息通常是与主要学习任务无关或冗余的，而蒸馏过程会使学生网络只保留与主要学习任务相关的知识，从而破坏水印信息
- 模型通过“记忆”来拟合噪声数据，而不是从中提取一般模式，并且在拟合噪声数据之前先学习真实数据的一般模式
  - 主任务和水印的表示之间的存在一个可以忽略不计的知识交集
    - 水印也可以看作为噪声

#### Computation Optimization

- 减少神经网络的计算时间
  - 通过低秩展开技术来近似卷积层，可能导致水印去除

### Backdoor Removal

- Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks
- 如果水印任务确实是一个与原始任务关联过于松散的后门函数
  - 可以通过针对神经网络的正常后门去除攻击来去除水印
- 后门攻击会使得神经网络对于某些异常输入（即包含触发器的输入）有着极端的敏感性，而正常的神经网络对于正常输入的敏感性应该是均匀分布的
  - 输入过滤（Input Filtering）：这种方法是在神经网络的输入层添加一个过滤器，用来检测和去除可能包含触发器的输入，从而防止后门攻击的激活
  - 神经元剪枝（Neuron Pruning）：这种方法是在神经网络的中间层或者输出层删除一些与触发器相关的神经元，从而破坏后门攻击的功能
  - 反向学习（Reverse Learning）：这种方法是在神经网络的输出层添加一个反向学习器，用来对抗触发器的影响，从而纠正后门攻击的错误输出
- Removing Backdoor-Based Watermarks in Neural Networks with Limited Data
  - WILD，它只使用少量的训练数据就可以有效地去除深度模型中的水印，而不影响模型在原始任务上的性能。

### Retraining

- Adversarial Frontier Stitching for Remote Neural Network Watermarking
- 模型可以适应潜在数据分布随时间的变化
  - 在重新训练时，水印可能会被损坏

### ideas

- 模型通过“记忆”来拟合噪声数据，而不是从中提取一般模式，并且在拟合噪声数据之前先学习真实数据的一般模式
- 主任务和水印的表示之间的存在一个可以忽略不计的知识交集
  - 水印也可以看作为噪声