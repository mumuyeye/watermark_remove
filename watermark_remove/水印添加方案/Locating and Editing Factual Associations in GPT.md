# Locating and Editing Factual Associations in GPT



## introduction

>- 分析了autoregressive transformer语言模型（如GPT）中**存储和回忆事实关联的机制**，发现这些关联对应于局部的、可直接编辑的计算过程
>- 开发了一种**因果干预方法**，用于**识别神经元激活**，这些激活在模型的事实预测中起决定性作用
>  - 他们发现，在处理主语token时，**中间层的前馈模块**有一组明显的步骤，介导事实预测
>
>- 使用Rank-One Model Editing (ROME)方法修改前馈权重，以更新特定的事实关联
>  - 证实了中间层前馈模块在存储事实关联中的重要作用，并暗示了直接操作计算机制可能是一种可行的模型编辑方法

## Interventions on Activations for Tracing Information Flow

>- 每个事实:表示为一个知识元组$t = (s, r, o)$，其中包含主语s、对象o和连接两者的关系r
>   - 提供一个自然语言提示p，描述了(s, r)，并检查模型对o的预测
>
>- 每一层根据前面的层计算出**全局注意力**和**局部MLP**的贡献
>   - 并且每个token关注其他token的先前状态
>      - 全局注意力是指每个词与其他词之间的相互作用
>      - 局部MLP是指每个词自身的非线性变换
>      - 对这两部分进行随机扰动，得到它们对于输出的因果作用
>         - 将全局注意力和局部MLP的因果作用相加，得到每一层对于输出的总因果作用，并与之前计算的结果进行比较

![image-20230911154451359](C:\Users\牧野\AppData\Roaming\Typora\typora-user-images\image-20230911154451359.png)

> - 计算每个模型组件（注意力和前馈网络）对事实预测的**平均间接效应**
>   - 在最后一层的最后一个token处，有一个“晚期位置”，在那里模型**生成事实对象**。这个位置的因果性很强
>   - 在中间层的最后一个主语token处，有一个“早期位置”，在那里模型**回忆事实关联**。这个位置的因果性也很强
>   - 在早期位置，**前馈网络**的贡献占主导地位（图2b），说明它们在存储事实关联中起重要作用
>   - 在晚期位置，**注意力机制**很重要（图2c），说明它们在生成事实对象时考虑了上下文信息

### Causal Tracing of Factual Associations

>观察模型在三种运行情况下的所有内部激活：
>
>- 一个clean的运行，正确地预测了事实；
>    - 将一个事实提示输入模型，并收集所有隐藏激活
>- 一个损坏的运行，模型的预测被破坏；
>    - 模型中的主语被遮蔽
>- 一个损坏-恢复的运行，测试单个状态恢复预测的能力
>    - 让模型在噪声嵌入上进行计算
>    - 选择一个特定的隐藏状态，将其恢复为原始的干净值，观察模型是否能够重新预测正确的事实对象

![image-20230911155248651](C:\Users\牧野\AppData\Roaming\Typora\typora-user-images\image-20230911155248651.png)

>（e）每个隐藏状态对预测事实对象的平均间接效应
>
>在处理**最后一个主语token**时，中间层的前馈 MLP 模块有很强的因果效应，而在处理**最后一个token**时，最后一层的注意力模块有很强的因果效应
>
>（f）仅前馈 MLP 激活对预测事实对象的平均间接效应
>
>在处理**最后一个主语token**时，中间层的前馈 MLP 激活有较强的因果效应，而在处理**其他token**时，前馈 MLP 激活的因果效应很弱
>
>（g）仅注意力激活对预测事实对象的平均间接效应
>
>在处理**最后一个主语token**时，注意力激活的因果效应很弱，而在处理**最后一个token**时，最后一层的注意力激活有很强的因果效应

![image-20230913100945177](C:\Users\牧野\AppData\Roaming\Typora\typora-user-images\image-20230913100945177.png)

>- 在处理主语的**最后一个token**时，**中间层的前馈 MLP 模块**有很强的因果效应，而在处理其他token时，**最后一层的注意力模块**有很强的因果效应
>    - 表明，在回忆一个事实时，模型中有两个重要的位置，分别对应于**事实关联的回忆和生成**
>- 在**早期位置**，前馈 MLP 模块的贡献占主导地位，说明它们在存储事实关联中起重要作用
>- 在**晚期位置**，注意力机制很重要，说明它们在生成事实对象时考虑了上下文信息
>- 在修改过的因果图中(下图)，没有未来 MLP 模块活动时，最低层失去了它们的因果效应，而更高层状态的效应几乎不依赖于 MLP 活动
>    - 这个结果证实了中间层 MLP 模块在回忆事实时的重要作用

<img src="C:\Users\牧野\AppData\Roaming\Typora\typora-user-images\image-20230913100525328.png" alt="image-20230913100525328" style="zoom: 67%;" />

### The Localized Factual Association Hypothesis

>每个中间层的 MLP 模块接受编码了主语的输入，然后产生回忆主题相关属性的输出。中间层 MLP 输出累积信息，然后在高层的注意力机制将信息复制到最后一个token。这个假设将事实关联定位在三个维度上
>
>- 即（i）在 MLP 模块中（ii）在特定的中间层上（iii）并且特别是在处理主语的最后一个token时

## Interventions on Weights for Understanding Factual Association Storage

### Rank-One Model Editing: Viewing the Transformer MLP as an Associative Memory

>- 第一步是选择一个向量 k∗ 作为主题 s 的查找键，方法是用 GPT 处理一些以 s 结尾的文本，然后取 MLP 模块输入的平均值
>    - 让 k∗ 能够在不同的上下文中保持稳定和一致
>- 第二步是选择一个向量 v∗ 作为关系 (r, o∗) 的值，方法是用**梯度下降**优化一个目标函数，使得当 v∗ 替换 MLP 模块输出时，GPT 能够根据事实提示 p 预测出正确的对象 o∗，同时保持对主题 s 的本质理解
>- 第三步是插入事实关联，方法是用一个闭式解来更新 MLP 模块的权重矩阵 W，使得 Wk∗ ​≈ v∗
>    - gf让 W 能够存储新的事实关联，而不破坏原有的事实关联

![image-20230911165441373](C:\Users\牧野\AppData\Roaming\Typora\typora-user-images\image-20230911165441373.png)

